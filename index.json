[{
    "title": "About",
    "date": "DOCUMENT LENGTH",
    "description": "Electrical Engineer with a focus on Machine Learning and Embedded Applications",
    "body": "Born from two immigrant parents, and the first in my family to go to college, I have always faced challenges while pursuing my goals. Thanks to an introduction to rocketry and electronics in high school through Systems GO, I discovered my passion for electronics. Today, through the world-class education of Texas A\u0026amp;M, I can pursue my goals further. This website is an exploration to showcase my favorite and most relevant academic and personal projects, as well as outlining my internship experience.\nWhen I am not coding or building tomorrow\u0026rsquo;s new buildings, I enjoy playing blues guitar, playing tennis, or talking with my brother, who is my biggest daily inspiration, and food. I really like food, likely a bit too much. Additionally, I enjoy racing, golf, and soccer as my sports choices, and I have been told I have a tendency to smile at everything even when upset. So far I have lived in Chicago(IL), Dallas(TX), College Station(TX), and Fredericksburg(TX), however, I am looking to travel more as soon as I can.\nI am looking to join an energetic team or company when I graduate Fall 2020, and I have no issue adapting quickly to any atmosphere. I am looking for careers in Embedded especially in Audio, Python/Machine Learning, Data Science, or Power. My professional platforms are listed under my profile picture on the left, so please don\u0026rsquo;t hesitate to reach out.\nI am definitely not a web designer so this website was built using Hugo, a static website builder. Learn more and contribute to the GitHub.\n",
    "ref": "/about/"
  },{
    "title": "Capstone: Machine Learning for Active Shooter events",
    "date": "DOCUMENT LENGTH",
    "description": "Using Batch Spectrograms and Image Recognition",
    "body": "Project Specifications Some Notes about capstone This project was built mainly using Python, Python packages, TensorFlow, and Raspberry Pi. All students in the Electrical Engineering program at Texas A\u0026amp;M must complete a sponsored 2-semester capstone program. This is a very involved program usually involving 30-40 hrs of work each week. Each week included constant reporting to supervisors, oral presentations, and laboratory work. While the project is a group project, here I talk about my work on the project, and how I helped the other members when needed. I received no help with the machine learning aspects of this project.\nFSR (Functional System Requirements) This is a shortened list since the actual FSR is over 20 pages long. I also list the FSR of the other components since it is relevant to how I work with other systems.\nThere were 3 members and therefore 3 main subsystems.\n Machine Learning Inference System Sound Location System End User Screen and Transmission System  The Requirements:\n The ASD (Active Shooter Detector) must differentiate when there is gunfire or not. The ASD must source the location of said noise in 3D space. The ASD must relay metrics from 1 and 2 to an officer through a screen. The ASD must be mounted on a rover and capable of rotational and/or translational movement, controlled by the screen mentioned in 3.  ICD (Interface Control Document) This is a graphical representation. The actual ICD specifies all of the possible standards and interconnection issues for each interface and is over 20 pages long.\n  System Overview: note interaction with other subsystems   Research Data collection My sponsor used the UMK-1 microphone to record gunfire at a local gun-range with various kinds of weapons. The reason that UMK-1 was chosen was because of its higher recording Frequency response compared to other USB plugs in microphones.\nThis batch of .wav files was then combined with a large open-source of .wav files which were an accumulation of \u0026ldquo;loud\u0026rdquo; sounds. I say \u0026ldquo;loud\u0026rdquo; because there was not a defined limit as to what \u0026ldquo;loud\u0026rdquo; meant.\nThe Model The next step was to pick a model for audio inference. Here we had to make a tough choice. Unlike common Linear Regressions, Audio has interesting constraints. For starters, it has more dimensionality, which restricts augmentation. How do you skew, flip, or stretch Audio without losing temporal fidelity?\nThe LSTM( Long and Short Term Memory) or BERT(Bidirectional Encoder Representation) is likely better suited to string and Natural Language Processing. But, due to time and frankly skill concerns, we decided to use a CNN image recognition model using a batch spectrogram to convert a time based .wav file, to a pixel + color-based .jpg image. In the end, we went with a slightly modified VGG 16 Image Classification model.\n  VGG-16 Architecture: Our input sizes were slightly different, but the layers were organized like above   Process Data Manipulation and the Batch Spectrogram Any good Data Scientist knows that preparation is 90% of the game, so I spent quite a bit of time arranging, labeling, and cleaning the data.\nTo train a VGG-16 there needs to be well-defined classes. The open-source sound data we chose came with .wav files with names such as \u0026ldquo;10120.wav\u0026rdquo;. This number is associated with a row value of the \u0026ldquo;1\u0026rdquo; column from an included .csv file. That same row tells you much more detailed information such as the sound source. There were around 10k samples a total of various lengths.\nTo accomplish the goal of organizing all of the files. I had to change the format of the .wav file to \u0026ldquo;sound_source + sample_number.wav\u0026rdquo;. As an example \u0026ldquo;10022.wav\u0026rdquo; is actually \u0026ldquo;Acoustic_guitar_22.wav\u0026rdquo;(this is the actual image in the header of the article). This was accomplished using Pandas. With Pandas, you can easily and non-recursively extract information into \u0026ldquo;column\u0026rdquo; based structures(i.e. DataFrames).\n#in what directory are our audio files? path = './Ambient_Audio' path_csv = './Ambient_Audio/_test_post_competition_scoring_clips.csv' #Read the files into an array audio_files audio_files = glob(path + '/*.wav') #adds anthing with extension .wav ####Read the CSV for the title content df = pd.read_csv(path_csv) df2 = df[['label','fname']] print(df.head()) print(df2.head()) Once I had all of the \u0026ldquo;correct\u0026rdquo; names stored in a DataFrame, I used Librosa to convert all of the images into batch spectrograms form. Batch Spectrograms allow you to have time, power, and frequency information in one picture.\n  MEL Spectrogram: This picture is a long timeline of multiple weapon shots   #y refers to amplitude #sr refers to sample rate #load a specific file audio_files[i] for i in range(len(audio_files)): #replace with length of y, sr = lr.core.load(audio_files[i]) #where y is an array representing the audio time series #where sr is the sampling rate and a scalar #to find the magnitude spectrogram S S = lr.feature.melspectrogram(y=y, sr=sr) #extracts all of the mfcc from y and the sample rate #Takes the fast fourier transform I believe? S_dB = lr.power_to_db(S, ref =np.max) #open a second figure #plt.figure(i) lr.display.specshow(S_dB, sr=sr, fmax=44000) #x_axis = 'time', y_axis = 'mel', ###Attaching the file names to filenames###### current_full_fname = audio_files[i] # pull just the last 8 elements of string also known as the label! #note that the .wav is also part of the label current_fname = current_full_fname[-12:] print(current_fname) ##See if that value is in the dataframe column 'label' df3 = df2.loc[df['fname'] == current_fname] final_fname = df3.iloc[0,0] plt.savefig( final_fname + '_'+ str(i) + '_mel.jpg' , pad_inches = 0, bbox_inches='tight', dpi=300) #plt.title(final_fname) #add a title Training on Super Computer Clusters No worries, the following sections are not nearly as long as the data manipulation sections.\nThrough my sponsor, I was able to gain access to the Olympus supercomputing clusters at A\u0026amp;M to Train my model. The classes were chosen as:\n multiple_gunshots not_gunshots single_shots  Because my model has RGB coloring, it is helpful to normalize data to prevent huge networks, which may or may not converge. The images were normalized from ~ 900x1400 px to 244px by 159px. I used matplotlib to scale down the image to as small as I could make it before I felt the images were too similar.\nFinally, I submitted everything in SLURM batch job files to Texas A\u0026amp;M, after installing all of the dependencies in my virtual python environment. There was a lot of obscure Linux to get everything to run, but in a few words, it was simply installing all dependencies and praying you didn\u0026rsquo;t mess up the batch file. It took a few tries before I understood the cluster.\nAt this point(Spring 2020), I had completed several ML projects already, so I had no issues with setting up or using TF(Tensorflow) or Keras. Below is just the layer and deployment code of one of the scripts.\nmodel = tf.keras.models.Sequential([ #Convolution and Max Pooling Layers tf.keras.layers.Conv2D(64, (3,3), activation = 'relu', input_shape = (244, 159, 3)), tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'), tf.keras.layers.MaxPooling2D(2,2), #1st tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'), tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'), tf.keras.layers.MaxPooling2D(2,2), #2nd tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'), tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'), tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'), tf.keras.layers.MaxPooling2D(2,2), #3rd tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), tf.keras.layers.MaxPooling2D(2,2), #4th # tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), # tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), # tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'), tf.keras.layers.MaxPooling2D(2,2), #5th #Lines the neurons up for connecting to dense Layers tf.keras.layers.Flatten(), #Drops some hidden layers in order to reduce overfitting tf.keras.layers.Dropout(.5), #Dense Layer fully connected tf.keras.layers.Dense(4096, activation = 'relu'), tf.keras.layers.Dense(4096, activation = 'relu'), #Final 3 output layer Gunshot, Multiple Shots, or no gunshot tf.keras.layers.Dense(3, activation = 'softmax') ]) # ### Compiling and Testing the Model #print the summary of our classes and of our model model.summary() #compile model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #Try and save every checkpoint which is every epoch checkpoint = ModelCheckpoint(\u0026quot;404_ver1_best_model.h5\u0026quot;,monitor='loss', verbose =1,save_best_only = True, mode= 'auto', period = 1) #save the history and set verbose to 1 to see live printout, for now at 0 if running on background server history = model.fit_generator(train_generator, epochs=1, validation_data = validation_generator, verbose = 1, callbacks = [checkpoint]) #Save the weights and biases of the trained model model.save(\u0026quot;404ver1.h5\u0026quot;) Model Layers: organized the classes\u0026rsquo; file tree by hand. Integration into an SBC Generally, unless you have particular needs, the model weights and biases are saved to an \u0026ldquo;.h5\u0026rdquo; file. Once saved, this file can be used to pass appropriate inputs and predict a class. The problem for us was how to extract this prediction information \u0026ldquo;real-time\u0026rdquo;, and then display the prediction to an end user live constantly. And, to do this processing on a RPi4(Raspberry Pi 4)\nTo do this I generated a script that constantly makes predictions in 3-second intervals. Although this is not an ideal \u0026ldquo;live prediction\u0026rdquo; it was the best we could do with our power constraints on the RPi4.\n  Continuous Prediction Loop: All the above are in a single script    # -*- coding: utf-8 -*- \u0026quot;\u0026quot;\u0026quot; Created on Sat Feb 22 18:09:51 2020 @author: Daniel \u0026quot;\u0026quot;\u0026quot; #---------------------RECORDING_RAW---------------- import sounddevice as sd from scipy.io.wavfile import write #---------------------SPECTROGRAM------------------ from glob import glob #For reading Files import librosa as lr #For audio analysis import librosa.display from librosa import display import numpy as np import matplotlib.pyplot as plt #for plotting #---------------------SPECTROGRAM------------------ from tensorflow.keras.models import load_model from keras_preprocessing import image from keras_preprocessing.image import load_img model = load_model(\u0026quot;404ver1.h5\u0026quot;) #How many times do you want to predict for i in range(0,5): #Live Continous Prediction using Librosa and a Pretrained Model #Read in a audio signal and save it to .wav format #function 'record' takes in no inputs and outputs #the directory to where the .wav file is saved def record(): #Our training audio is 44Khz fs = 44000 #Let's say 3 seconds, we will pipeline seconds = 3 #Record 3 seconds and save it current_recording = sd.rec(int(seconds*fs), samplerate = fs, channels = 2) #Wait until finished sd.wait() #Save as wav with approopriate size. write('last_3_seconds.wav', fs, current_recording) #Name the path of said recording as a string path_to_wav = './last_3_seconds.wav' return path_to_wav #After recording and saving the wav file, convert it into the #spectrogram image. 'spectro' has input path and output path_to_spectro def spectro(path_to_wav): #y refers to amplitude #sr refers to sample rate #load a specific file audio_files[i] y, sr = lr.core.load(path_to_wav) #extracts all of the mfcc from y and the sample rate #to find the magnitude spectogram S S = lr.feature.melspectrogram(y, sr=sr) #Takes the fast fourier transform I believe? S_dB = lr.power_to_db(S, ref=np.max) lr.display.specshow(S_dB,sr=sr, fmax=44000) plt.savefig('last_3_spectrogram.png', pad_inches = 0, bbox_inches='tight', dpi=300) plt.close() #Name the path of said figure path_to_spectro = './last_3_spectrogram.png' return path_to_spectro #After converting the wav to spectrogram, run an inference on it def predict(X): #Load images from file img = load_img(X , target_size = (1402,913) ) #Turn into array for parsing img = image.img_to_array(img) #expand the dimensions horizontally, turn into one big line x = np.expand_dims(img, axis=0) #now stack it all vertically img = np.vstack([x]) #use built model subfunctions to produce a prediction classes = model.predict_classes(img, verbose = 0) percent_classes = model.predict(img, verbose = 0) print(classes, '\\n', percent_classes) return classes, percent_classes path_to_wav = record() path_to_spectro = spectro(path_to_wav) print(predict(path_to_spectro)) Finally, the prediction is displayed using RPi4\u0026rsquo;s camera module. Using some Picamera functions, I helped the other subsystems to display live predictions on a screen.\n  Live Camera Output: As you can see the probability of a gunshot was low at this point    Results As a group, we scored an 89%, mainly because they have a policy of not giving As unless the project is sold to a company. As for the ML here are the metrics. The truth is more data and cleaner data is needed. Because of University and Safety policies we only collected around 100 or so samples of gunfire. Therefore the model had trouble finding patterns converging to gunshot classification. Although a high accuracy is shown, it is highly skewed by the not gunshot class. Simply put, I should have trained either on LSTM or by only having single shots as my only class.\nAlso unfortunate, Covid-19 interfered with finishing the project. Mainly this hindered mounting the system on a rover and transmitting information OTA(Over the Air) through Bluetooth. When progress was canceled, everything was \u0026ldquo;hardwired\u0026rdquo; still.\n  Metrics: Includes loss value checkpoints    Conclusion In general terms, this is the project I am most proud of. And as it is my capstone, a high point in my college education. Using the latest technology in bleeding-edge labs at Texas A\u0026amp;M is always a pleasure. I would like to thank my, for now, unnamed sponsor, Samuel Beauchamp, and Jacob Beckham for being amazing partners.\n",
    "ref": "/projects/machine-learning/"
  },{
    "title": "Contacter",
    "date": "DOCUMENT LENGTH",
    "description": "",
    "body": "",
    "ref": "/contact/"
  },{
    "title": "Coulomb Force predictions from chemical makeup",
    "date": "DOCUMENT LENGTH",
    "description": "Python, Pandas, and Chemistry Project",
    "body": "Project Details This is the final project for ECEN 489, an elective course in Computational Data Science. This class is taught by Dr. Jin Tao, and incredible researcher at the high computing center at Texas A\u0026amp;M.\nMeasurements at the atomic level are incredibly difficult, but of extreme relevance in the study of material sciences. One of the fundamental Measurements is that of the Coulomb Force. It is an equation that gives you a total force of 2 electrical charges affecting each other. It is straight forward for 2 atoms but increases in difficulty exponentially in 3D space and with multiple atoms in a molecule.\nThe question was then if we could create a prediction of total Coulomb force, based exclusively on the atomic formula of a molecule. This maybe wouldn\u0026rsquo;t be as accurate, but it may allow predictions for molecules so large and complex it is prohibitive to measure.\nProcess The first process is to visualize and understand the data. I used the PUBCHEM website to check my work and to obtain the data of simple molecules. The data was presented in JSON, and I used Pandas to import the data into DataFrames and extract what I needed. I then had to use a massive amount of string operators to create an XYZ file, which describes the chemical makeup of the molecule.\n   Name X Y Z      O -0.0782 -1.5651000000000002 1.3894    O -2.2297000000000002 -0.9343 -1.3664    O 2.2983000000000002 -1.2575 -0.2803    O 2.8296 0.9664 -0.325    C -0.30260000000000004 -0.9556 0.1153    C -1.8113000000000001 -0.6388 -0.0329    C 0.5877 0.2525 0.013800000000000002    C -2.18 0.7909 0.2585    C 0.132 1.5099 0.1068    C -1.268 1.77 0.33490000000000003    C 2.0222 0.061700000000000005 -0.2141    H -0.041800000000000004 -1.6945000000000001 -0.6527000000000001    H -2.3841 -1.2908 0.6377    H -3.2301 1.0276 0.4037    H 0.7787000000000001 2.3781 0.0315    H -1.5681 2.7917 0.54    H 0.8454 -1.8688 1.4156    H -3.1904 -0.7899 -1.4159000000000002    H 3.2524 -1.4274 -0.4325     Once I had the XYZ data, I used called PC3 to view individual molecules to check against the PubChem data.\nWith this Data, I can use an equation known as the Coulomb matrix equation to coulomb to create a matrix that represents the forces between in all of the atoms in a molecule. I then use the XYZ data I pulled to find if there was a comparison.\nConclusion Ultimately, we ran out of time to complete a machine learning model, but this was a still pretty intense exercise in python and pandas. However, it is worth noting that there didn\u0026rsquo;t seem to be a direct connection from chemical makeup, to Coulomb force in a conventional correlation measurement. However, this was a project where I was able to pull data from a complex data set, clean it, and use operators to convert it into a file to be used by an open-source program.\n",
    "ref": "/projects/molecule_linear_regression/"
  },{
    "title": "Microstrip Simulation",
    "date": "DOCUMENT LENGTH",
    "description": "Using MATLAB's GUI builder and Recursive methods",
    "body": "Project Specifications This is a MATLAB and MATLAB app builder project. It was part of ECEN 445, an elective course in Applied Electromagnetics.\nThe mathematical concept is an exercise in recursive methods. There is a layout of nodes on an X and Y plane. The Z plane is the measure of Potential in volts.\nIn the middle, there is a simulated microstrip line that reaches about halfway into the viewable area. That theoretical line is of width 0 and has a 1-volt current running through it.\nThere is a perfect conductor wall the surrounds the area of study. The voltage of this wall is 0.\nThe Requirements:\n  Create a script that updated each node value, until the difference between the updated and previous nodes value was less than .001 Volts.\n  Create a UI that would allow a user to make a simulation with as many aspect ratios and nodes as they desired.\n  Process Back-End The voltage value of each node is affected by the nodes around it, described by an equation that contains the last value of the current node, and the current values of the nodes around it. Therefore this equation was put into a MATLAB script and called upon when needed. Note that this equation needs multiple iterations before a minimal difference is observed.\nI wrapped that equation inside a function for calling as needed. Then with given inputs, I first create a grid, then use the said equation to update the nodes from top left to bottom right.\nfunction [countIter, alpha, omega, cap] = FDSOR_Generation(a,b,d,w,n_a,n_b,e_r,threeD) %% SET PARAMETERS %Let's use a common denominator of 6 for d %test case number 1. %initial parameters f = uifigure; u = uiprogressdlg(f,'Title','Running FDSOR on Chosen Geometry', 'Message','Loading ','Cancelable','on'); %convTol = .00001;%convergance tolerance convTol = .00001; wd = w / d; bd = b / d; aw = a / w; %horizontal distances h = a / n_a; %vertical distances k = b / n_b; %ratio alpha = h/k; %optimal relaxation omega = 2*(1 - (pi/sqrt(2))*sqrt(((1/n_a^2) + (1/n_b^2)))); %% GENERATE BOX and STRIPLINE %Outputs will be the %optimal relax factor %The final capacitance %The plot of potentials %%% Generate Potentials %%% %First columns tracks X Boundary %Second columns tracks Y Boundary x = [0;a;a;0]; y = [0;0;b;b]; %Generate the boundary points %plot(x,y, 'k', 'Linewidth', 3); hold off %stiill need to superimpose points %Generate the stripline x= [0;w]; y = [6,6]; %plot(x,y, 'k', 'Linewidth',3) %% GENERATE NODES %x values go from 0 to the length 'a' %in intervals of 'h' xx = 0:h:a; %y values go from 0 to height 'b' %in intervals of k yy = 0:k:b; %use th meshgrid function to get an xy grid [X,Y] = meshgrid(xx,yy); %Each row is a copy of X %Each column is a copy of Y %pull the individual coordinates and store them %into columns X and Y PotCoord = [X(:),Y(:)]; ZeroPots = zeros(size(PotCoord,1),1); PotCoord = [PotCoord,ZeroPots]; %plot(PotCoord(:,1), PotCoord(:,2), 'o') %% GENERATE NODE POTENTIALS %initialze the 3rd col of PotCoord to 0 %This is basically initizaling all the voltages to 0 close all %We iterate through all but one \u0026quot;layer\u0026quot; of the boxes forms %this will maintain the outside nodes at zero always % for jj = k:k:b-k % for ii = h:h:a-h Residuals = zeros(1,size(PotCoord(:,1), 1));%Preallocate Residual Rmax = 1; %initialize the max residual for now ie 1 volt countIter = 0;%Count Iterator idxRes = 0;%Address Pointer of Residuals while (Rmax \u0026gt; convTol) if u.CancelRequested break end if countIter == 1000 break end for jj = k:k:b-k for ii = h:h:a-h %Find the CURRENT Node Potential %store all of the elements %of PotCoord where the first column is %our current x, and all the elemnents where the second %column is our current y %Extract the row into tmp try tmpCurr = [ii,jj]; %what pt am I looking for %find a row that closes matches that from my grid matrix [~, idxCurrent] = ismembertol(tmpCurr, PotCoord(:,[1:2]), 'ByRows', true); %store that row which is [x,y,potential] tmpCurr = PotCoord(idxCurrent,:); %Extract just the potential from that CurrPot = tmpCurr(:,3); catch end try tmpRight = [ii+h,jj]; [~, idxRight] = ismembertol(tmpRight, PotCoord(:,[1:2]), 'ByRows', true); tmpRight = PotCoord(idxRight,:); RightPot = tmpRight(:,3); catch %If the above returns an empty, then you are outside the box %therefore the potentials should be zero RightPot = 0; end try tmpLeft = [ii-h,jj]; [~, idxLeft] = ismembertol(tmpLeft, PotCoord(:,[1:2]), 'ByRows', true); tmpLeft = PotCoord(idxLeft,:); LeftPot = tmpLeft(:,3); catch LeftPot = 0; end try tmpAbove = [ii,jj+k]; [~, idxAbove] = ismembertol(tmpAbove, PotCoord(:,[1:2]), 'ByRows', true); tmpAbove = PotCoord(idxAbove,:); AbovePot = tmpAbove(:,3); catch AbovePot = 0; end try tmpBelow = [ii,jj-k]; [~, idxBelow] = ismembertol(tmpBelow, PotCoord(:,[1:2]), 'ByRows', true); tmpBelow = PotCoord(idxBelow,:); BelowPot = tmpBelow(:,3); catch BelowPot = 0; end %Update the current potential %Here is function that finds the new potential R_p = FindNewPot(LeftPot,RightPot,AbovePot,BelowPot,e_r,alpha,tmpCurr); R_p = R_p - CurrPot; %Which point do we want to update? tmpCurr %find that row inside PotCoords and pull the index [~, idx] = ismembertol(tmpCurr, PotCoord, 'ByRows', true); %Now update that point with our new potential %Recall that the third column holds the potentials. PotCoord(idx,3) = CurrPot + omega * R_p; %Before we go on to the next node, we should check if our current %index is actually on the stripline i.e. ii \u0026lt;= w, and jj == d if (tmpCurr(:,1) \u0026lt;= w) \u0026amp;\u0026amp; (tmpCurr(:,2) == d) PotCoord(idx,3) = 1; end %Now we update the residuals %Residuals = [Residuals, R_p]; idxRes = idxRes + 1; Residuals(idxRes) = R_p; end end %We check the residuals matrix to figure out when to stop iterating idxRes = 0; Rmax = max(Residuals); u.Message = 'Current Max Residual ' + string(Rmax); %Residuals = []; %clear Residuals countIter = countIter + 1; end %% PLOTTONG if threeD == 1 figure plot3(PotCoord(:,1),PotCoord(:,2),PotCoord(:,3),'o'); xlabel('x') ylabel('y') zlabel('potential (v)') savefig(string(n_a)+ string(countIter) + '.fig'); end %% FIND CAPACITANCE nb = d - 3*k; %Bottom Left Y na = d + 3*k; %Top Left Y ns = d; %height of the stripline mr = [w + 3 * h]; %Right most plane %sum from 0 to mr in steps of h %you need two potentials at each point the node above and below %current (m, nb+ k) (nb - k) LatSums = 0; for ii = 0:h:mr %find the potntial above the contour nb AboveCBpot = [ii,nb+k]; [~,idxAboveCBpot] = ismembertol(AboveCBpot,PotCoord(:,[1:2]),'ByRows',true); A = PotCoord(idxAboveCBpot,3); %find the potential below the contour nb BelowCBpot = [ii,nb-k]; [~,idxBelowCBpot] = ismembertol(BelowCBpot,PotCoord(:,[1:2]),'ByRows',true); B = PotCoord(idxBelowCBpot,3); %find the potntial above the contour na AboveCApot = [ii,na+k]; [~,idxAboveCApot] = ismembertol(AboveCApot,PotCoord(:,[1:2]),'ByRows',true); C = PotCoord(idxAboveCApot,3); %find the potential below the contour na BelowCApot = [ii,na-k]; [~,idxBelowCApot] = ismembertol(BelowCApot,PotCoord(:,[1:2]),'ByRows',true); D = PotCoord(idxBelowCApot,3); %Find the lateral capacitance a this locatation %Sum them up LatSums = LatSums + (e_r*(A-B) + (C-D)); end VertSumBottom = 0; for ii = nb:k:ns LeftCPot = [mr+h, ii]; [~,idx] = ismembertol(LeftCPot,PotCoord(:,[1:2]),'ByRows',true); A = PotCoord(idx,3); RightCpot = [mr-h, ii]; [~,idx] = ismembertol(RightCpot,PotCoord(:,[1:2]),'ByRows',true); B = PotCoord(idx,3); VertSumBottom = VertSumBottom + (e_r*(A) - (B)); end VertSumTop = 0; for ii = ns:k:na LeftCPot = [mr+h, ii]; [~,idx] = ismembertol(LeftCPot,PotCoord(:,[1:2]),'ByRows',true); A = PotCoord(idx,3); RightCpot = [mr-h, ii]; [~,idx] = ismembertol(RightCpot,PotCoord(:,[1:2]),'ByRows',true); B = PotCoord(idx,3); VertSumTop = VertSumTop + ((A) - (B)); end cap = -1* (alpha*(LatSums) + (alpha^-1)*(VertSumBottom + VertSumTop)); %% SAVE WORKSPACE save(string(n_a)+ string(countIter) + '.mat') %% UPDATE POTENTIAL FUNCTION function NewPot = FindNewPot(LeftPot,RightPot,AbovePot,BelowPot,e_r,alpha,tmpCurr) %First we should check if we are inside interface or not. if (tmpCurr(:,2) \u0026lt;= d) %e_r should be the user input else e_r = 1; end A = (2*(1+alpha^2))^-1; B = (2*alpha^2)/(1+e_r); NewPot = A*(LeftPot + RightPot + B*(AbovePot + e_r*BelowPot)); end end Front-End Using MATLAB\u0026rsquo;s app builder and API like interface, I created an interface for a user to custom make a simulation. The main challenge was connecting this to the script that ran the simulation. Secondary, but still as difficult was making sure that the user ran the program correctly. To do this, I used button locks, indicator lights, warning messages, prompts, and more to make it as easy as possible.\nI also stuck to a linear methodology of design, meaning I directed the user linearly. Finally, once a simulation was started, I indicated how many iterations were done, and the current tolerance level as a way of indicating how long it might run. Tolerance in this case meaning the biggest change that was made from one iteration to another from all the nodes. Once this was less than .001 Volts, the simulation was finished.\n  User Interface UI/UX:     Live Simulation: Residual is also known as tolerance   Conclusion While receiving an A was great, my biggest takeaway from this project was a deep understanding of the workflow needed to build a MATLAB UI, as well as plenty of practice with MATLAB itself.\nOnce the simulation was complete, a 3d plot was generated if so desired.\n  Simulation: With different parameters     Simulation: With different parameters   ",
    "ref": "/projects/microstrip_simulation/"
  },{
    "title": "Solar Power: Linear Regression Utility",
    "date": "DOCUMENT LENGTH",
    "description": "Using Python's GUI builder and Tensorflow to make ML accessible. Creating a tool to estimate the effect of Solar Panel installations",
    "body": "Project Specifications This was a completely optional project for the Data Science for Energy and Power class at Texas A\u0026amp;M. In addition to machine learning, this class focused on time predictive techniques and methodology such as ARMA, SVD, and K-means.\nAs many consumers begin to move to solar, it is hard to accurately measure the impact of the number of panels vs the environmental benefits a solar array creates. Many companies might stretch the truth about efficacy, while others may be more conservative in their estimates.\nTherefore, the goal was to create a predictive model, that taking an input of solar panels to be installed (consumer panels have a standard size), could predict the carbon offset in metric tons of an installation. The model gets its prediction not by some set formula, but from a trained linear regression on the data provided by Google\u0026rsquo;s Project Sunroof\nA secondary goal was to make this prediction tool user friendly, which involved creating a GUI that would allow anyone to see predictions.\nProcess The first process is to obtain the data and to extrapolate the things that I need. Here is a snippet of the original table. It is organized by cities.\n   region_name state_name lat_max lat_min lng_max lng_min lat_avg lng_avg yearly_sunlight_kwh_kw_threshold_avg count_qualified percent_covered percent_qualified number_of_panels_n number_of_panels_s number_of_panels_e number_of_panels_w number_of_panels_f number_of_panels_median number_of_panels_total kw_median kw_total yearly_sunlight_kwh_n yearly_sunlight_kwh_s yearly_sunlight_kwh_e yearly_sunlight_kwh_w yearly_sunlight_kwh_f yearly_sunlight_kwh_median yearly_sunlight_kwh_total carbon_offset_metric_tons existing_installs_count     NULL Pennsylvania 40.70188 40.61294 -75.4622 -75.5461 40.65035 -75.4958 985.15 7673 98.72285 84.84078 39701 113097 83193 87582 203964 32 527537 8 131884.3 10025162 35149706 23041858 24541877 59927475 9215.539 1.53E+08 97025.99 11   NULL NULL 32.55156 32.54226 -116.92 -117.03 32.54845 -116.957 1300.5 2 100 66.66667 13 18 0 0 0 13 31 3.25 7.75 4745.559 7692.078 0 0 0 5209.465 12437.64 0 0   Aberdeen North Carolina 35.18396 35.05361 -79.3885 -79.5383 35.1436 -79.4247 1083.75 1078 86.0984 71.62791 3248 16681 13821 12775 92404 38 138929 9.5 34732.25 916858.5 5550471 4255421 3855701 30274566 11665.38 44853017 26330.81 0   Abilene Texas 32.61433 32.23666 -99.5896 -100.086 32.43502 -99.7507 1252.411 42802 97.87507 93.67504 172303 586619 403010 557991 1718095 42 3438018 10.5 859504.5 56039759 2.34E+08 1.4E+08 2.04E+08 6.44E+08 15695.83 1.28E+09 621366.8 25    First, I got rid of all of the nulls and pulled the two columns I was interested in\u0026hellip; number_of_panels_total and carbon_offset_metric_tons. I retrospect, I should have checked to see how many nulls there were, and if this was an appropriate measure to take.\n   number_of_panels_total carbon_offset_metric_tons     527537 97025.99147   31 0   138929 26330.80621   3438018 621366.8494   175853 21479.14411   170469 31333.36638   31219 6075.026699    I plot this against each-other to see the relationship (cover picture)   Relationship between number of panels and carbon offset.   It should be shown in the graph, but the carbon offset is in metric tons.\nThe model Using these two columns, I used Numpy, Keras, and TensorFlow to create a linear regression model. The code below is almost guided by the TF docs. It probably would have had an easier implementation just making my own, but the benefit of this code is that you could associate on output with multiple inputs, which again, I didn\u0026rsquo;t need.\nimport matplotlib.pyplot as plt import numpy as np import pandas as pd import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import tensorflow_docs as tfdocs import tensorflow_docs.modeling import tensorflow_docs.plots from tensorflow.keras.models import load_model #import the data from an excel/csv file myFile = pd.read_csv(r'E:\\Daniel\\Documents\\_TAMU\\_SPRING 2020\\ECEN_489_POWER\\_Solar_Project\\city.csv', encoding = 'utf-8') #turn the imported data into a dataframe df1 = pd.DataFrame(myFile) #extract the carbon offset datak carbon_offset = df1[['carbon_offset_metric_tons']] #extract the region data region = df1[['region_name']] #extract the number of panels panels = df1[['number_of_panels_total']] # combine carbon offset and panel data, drop nan columns data_set = pd.concat([panels,carbon_offset], axis = 1) data_set = data_set.dropna() #Split the data train_dataset = data_set.sample(frac=.8, random_state=0) test_dataset = data_set.drop(train_dataset.index) #set carbon offset as the Y output Y_train = train_dataset.pop('carbon_offset_metric_tons') Y_test = test_dataset.pop('carbon_offset_metric_tons') #normalization norm_train_dataset = (train_dataset-train_dataset.mean())/train_dataset.std() norm_test_datatest = (test_dataset-train_dataset.mean())/train_dataset.std() norm_Y_train = (Y_train-Y_train.mean())/Y_train.std() norm_Y_test = (Y_test-Y_test.mean())/Y_test.std() print(\u0026quot;mean X = \u0026quot; + str(train_dataset['number_of_panels_total'].mean())) print(\u0026quot;std X = \u0026quot; + str(train_dataset['number_of_panels_total'].std())) print(\u0026quot;std Y= \u0026quot; + str(Y_train.std()) ) print(\u0026quot;mean Y= \u0026quot; + str(Y_train.mean()) ) plt.scatter(data_set[['number_of_panels_total']], data_set[['carbon_offset_metric_tons']]) plt.title('panels vs. carbon offset') plt.ylabel('carbon_offset') plt.xlabel('number of panels') plt.savefig('panels_v_carbon.png') plt.show() print(train_dataset.head(10)) #MODEL DEFINITIONS def build_model(): model = tf.keras.models.Sequential([ tf.keras.layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]), tf.keras.layers.Dense(64, activation='relu'), tf.keras.layers.Dense(1) ]) optimizer = tf.keras.optimizers.RMSprop(0.001) #learning rate model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse']) return model model = build_model()#build the structure of the model EPOCHS = 1000 #train the model 1000 iterations stop_training = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50) history = model.fit( norm_train_dataset, norm_Y_train, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[stop_training, tfdocs.modeling.EpochDots()]) #model.save('solar_model.h5') #evaluate the model using the testing set loss, mae, mse= model.evaluate(norm_test_datatest,norm_Y_test,verbose=2) print(str(mae)) #use history function to plot accuracy train_acc = history.history['mae'] val_acc = history.history['val_mae'] epochs = range(len(train_acc)) plt.plot(epochs, train_acc, 'r', label='Mean Absolute Error') plt.title('Error Decrease per Epoch') plt.legend(loc=0) plt.xlabel('Epochs') plt.ylabel('MAE') plt.savefig('Loss_Metrics.png') plt.show() The GUI Once I had the produced h5 model, I used Tkinter to create the python application.\n  Overview of Solar Panel app   from tkinter import * import matplotlib.pyplot as plt import numpy as np import pandas as pd import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers import tensorflow_docs as tfdocs import tensorflow_docs.modeling import tensorflow_docs.plots #Import the script for loading a module from load_model import load_solar_model #The input from user requires some preprocessing def prep_user_input(txt_in): txt_cleaned = ''.join(i for i in txt_in if (i.isdigit() or i == '.')) try: new_text = float(txt_cleaned) except ValueError: print('ERROR: Please enter a real numerical value!') predict_input = np.array([new_text]) predict_input = predict_input[:1] # print(predict_input) return predict_input, txt_cleaned #make all of my fonts the same myfont = \u0026quot;Circular Std bold\u0026quot; #Where is the model currently saved mypath = 'E:\\Daniel\\Documents\\_TAMU\\_SPRING 2020\\ECEN_489_POWER\\_Solar_Project\\CodeFiles\\solar_model.h5' #open the main window loop window = Tk() window.title(\u0026quot;Solar Panels Carbon Offset Estimation App\u0026quot;) #window.geometry('700x700') #BUTTON CLICKED ACTION def run_prediction(): #when button is clicked, gather what was in the text field number_of_panels = txt.get() #prep it through formatting for loading to the machine learning model predict_input, txt_cleaned = prep_user_input(number_of_panels) #show the formatted array input to the user b.configure(text=\u0026quot;Calculating Carbon Offset for: \u0026quot; + txt_cleaned + \u0026quot; panels\u0026quot;) #change text to calulating print(predict_input) #send it to load_model for evaluating out_prediction = load_solar_model(predict_input, mypath) #Show the prediction pred_txt.configure(text = 'Predicted Carbon Offset(metric tons): ' + np.array2string(out_prediction)) #..... #DECRIPTIVE TEXT b = Label(window, text ='Enter number of solar panels:', font=(myfont,10)) b.grid(column=1, row=0) #TEXT INPUT txt = Entry(window, width = 50) txt.grid(column= 1, row = 1) #MAIN CALCULATE BUTTON calculate_btn = Button(window, text = \u0026quot;PREDICT\u0026quot;, command = run_prediction, font=(myfont,10)) calculate_btn.grid(column = 1, row = 2) #SHOW THE OUTPUT FOR GIVEN PREDICTION pred_txt = Label(window, text ='Predicted carbon offset: ', font = (myfont,10)) pred_txt.grid(column = 1, row = 3) window.mainloop() import matplotlib.pyplot as plt import numpy as np import pandas as pd import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import load_model import tensorflow_docs as tfdocs import tensorflow_docs.modeling import tensorflow_docs.plots def load_solar_model(number_panels, myPath): #load saved model carbon_model = load_model(myPath) #evaluate input into your model est_carbon_offset = carbon_model.predict(number_panels) return est_carbon_offset #This will go into further parameters #file path is user determined # filepath = 'E:\\Daniel\\Documents\\_TAMU\\_SPRING 2020\\ECEN_489_POWER\\_Solar_Project\\CodeFiles\\solar_model.h5' # number_panels = np.array([.3]) # cb_off = load_solar_model(number_panels[:1], filepath) # print(number_panels) # print(str(cb_off))   GUI before entry     GUI after entry   Conclusion Using MAE as our loss value because the values were so large MSE might be a bad measure, we reached 92% accuracy or 8% MAE. Generally, the project was a success. In the future, I would like to develop this further for municipalities that may be looking at solar power.\n",
    "ref": "/projects/solar_linear_regression/"
  },{
    "title": "Traffic Stop Racial and Gender Bias in Texas",
    "date": "DOCUMENT LENGTH",
    "description": "Python, Pandas, and Matplotlib Project. Analyzing bias in Texas when pulled over for similar crimes.",
    "body": "Project Specifications This was the midterm project for a class on Computational Data Science taught by Dr. Jin Tao, a Data Science Guru and excellent researcher at Texas A\u0026amp;M.\nAs someone who has personally been affected by both systemic and explicit racism, I chose this project with a real interest in the finds. The goal of this project was to identify biases in policing during traffic stops during 2010-2016 in TEXAS.\nThe question was\u0026hellip;do different groups of people receive citations rather than warnings in a disproportionate manner for the same type of traffic stop.\nProcess First was simply opening the data. The only way was to use Pandas\u0026rsquo; read_csv function into Python. Here is a snippet of the data that was chosen.\n    id state stop_date stop_time location_raw county_name county_fips fine_grained_location police_department driver_gender driver_age_raw driver_age driver_race_raw driver_race violation_raw violation search_conducted search_type_raw search_type contraband_found stop_outcome is_arrested lat lon officer_id driver_race_original     0 TX-2010-0000002 TX 1/1/2010 0:00 Guadalupe Guadalupe County 48187 622  F   Asian Asian Speeding Over Limit (#) Speeding FALSE   FALSE Warning  29.62287 -97.7787 11524 Asian   1 TX-2010-0000003 TX 1/1/2010 0:00 Fannin Fannin County 48147 668  F   White White Speeding Over Limit (#) Speeding FALSE   FALSE Warning  33.60318 -96.1502 12274 White   2 TX-2010-0000004 TX 1/1/2010 0:00 Coryell Coryell County 48099 560  M   Black Black Fail to Maintain Financial Responsibility (#) Paperwork FALSE   FALSE Citation  31.1216 -97.8354 12365 Black    The first thing was to get rid of null data. I should have detailed how many null points there was, however from visual inspection there was not that many. Really the main null cells, were a few where the race of the driver was not recorded. Next I pull just the rows that contained Race, and violations that were similar\u0026hellip; i.e. included the word \u0026ldquo;speeding\u0026rdquo;. Additionally, I pull the \u0026ldquo;registered\u0026rdquo; gender Now I also crucially grab the column that shows the \u0026ldquo;Outcome\u0026rdquo; of the traffic stop. It basically shows whether the outcome was a citation or a ticket.\n   driver_gender violation stop_outcome driver_race_original     M Speeding Warning Asian   M Speeding Warning White    Armed, pun-intended with this information, I can do a lot of analysis. I used MatplotLib to generate charts and graphs.               Conclusion I was surprised to see the highest percentage belonged to those classified of Asian descent. However, it\u0026rsquo;s important to note very few Asian drivers are pulled over. Another layer to this investigation missing was to see the number of traffic stops to the actual population of that group. This may have shown bias from a different angle. What is clear is that Males are pulled over in much higher amounts the Females. I cannot accurately speculate on the reasoning behind this, but there are plenty of studies about it.\n",
    "ref": "/projects/traffic_stop_gender_bias/"
  }]
