<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<title>Capstone: Machine Learning for Active Shooter events - Daniel Dominguez Chavez</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="generator" content="Hugo 0.72.0" /><meta itemprop="name" content="Capstone: Machine Learning for Active Shooter events">
<meta itemprop="description" content="Using Batch Spectrograms and Image Recognition">

<meta itemprop="wordCount" content="2077">
<meta itemprop="image" content="https://github.com/d9dch/Daniel_Chavez/img/new_images/guitar_sound.jpg">
<meta itemprop="image" content="https://github.com/d9dch/Daniel_Chavez/img/new_images/sys_over.jpg">



<meta itemprop="keywords" content="Python,Pandas,Tensorflow,Audio Engineering,Embedded,Linux,RPi4,Teensy,Keras," /><meta property="og:title" content="Capstone: Machine Learning for Active Shooter events" />
<meta property="og:description" content="Using Batch Spectrograms and Image Recognition" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://github.com/d9dch/Daniel_Chavez/projects/_machine-learning/" />
<meta property="og:image" content="https://github.com/d9dch/Daniel_Chavez/img/new_images/guitar_sound.jpg" />
<meta property="og:image" content="https://github.com/d9dch/Daniel_Chavez/img/new_images/sys_over.jpg" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://github.com/d9dch/Daniel_Chavez/img/new_images/guitar_sound.jpg"/>

<meta name="twitter:title" content="Capstone: Machine Learning for Active Shooter events"/>
<meta name="twitter:description" content="Using Batch Spectrograms and Image Recognition"/>
<link rel="stylesheet" href="https://github.com/d9dch/Daniel_Chavez/css/bundle.min.759e3848ea529eef056dab665b241e2af569e601ab80f68f8d4a4d002a155dd4.css" integrity="sha256-dZ44SOpSnu8FbatmWyQeKvVp5gGrgPaPjUpNACoVXdQ="><link rel="stylesheet" href="https://github.com/d9dch/Daniel_Chavez/css/add-on.css">
</head>

  <body>
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="https://github.com/d9dch/Daniel_Chavez/" class="nav">
        
          
            projects
          
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          
            <a href="https://github.com/d9dch/Daniel_Chavez/" class="nav link"><i class='fa fa-home'></i> Home</a>
          
        
      
        
          
          
            <a href="https://github.com/d9dch/Daniel_Chavez/about/" class="nav link"><i class='far fa-id-card'></i> About</a>
          
        
      
        
          
          
            <a href="https://github.com/d9dch/Daniel_Chavez/projects/" class="nav link"><i class='far fa-newspaper'></i> Projects</a>
          
        
      
        
          
          
            <a href="https://github.com/d9dch/Daniel_Chavez/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
          
        
      
      <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/fr" lang="fr" class="nav no-lang link">Français (fr)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/pl" lang="pl" class="nav no-lang link">Polski (pl)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/pt" lang="pt" class="nav no-lang link">Português (pt)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/de" lang="de" class="nav no-lang link">Deutsche (de)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/es" lang="es" class="nav no-lang link">Española (es)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/zh-cn" lang="zh-cn" class="nav no-lang link">中文 (zh-cn)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/zh-tw" lang="zh-tw" class="nav no-lang link">中文 (zh-tw)</a>
      
    
      
        <a href="https://github.com/d9dch/Daniel_Chavez/ja" lang="ja" class="nav no-lang link">日本語 (ja)</a>
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      





    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="https://github.com/d9dch/Daniel_Chavez/"><img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/headshot_front.jpg" class="circle" width="" alt="Hugo Future Imperfect Slim" /></a>
  <header>
    <h1>Daniel Dominguez Chavez</h1>
  </header>
  <main>
    <p>Electrical Engineering Senior at Texas A&M</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/pacollins/hugo-future-imperfect-slim" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/example" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>













<li><a href="//instagram.com/example" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>













<li><a href="mailto:example" target="_blank" title="Email" class="far fa-envelope"></a></li>

      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/_machine-learning/">Capstone: Machine Learning for Active Shooter events</a></h2>
    
    
      <p>Using Batch Spectrograms and Image Recognition</p>
    
  </div>
  <div class="meta">
    
    <time class="published" datetime="0001-01-01 00:00:00 &#43;0000 UTC">
          DOCUMENT LENGTH
    </time>
    <span class="author"></span>
    
      <p>10 minute read</p>
    
  </div>
</header>

    <section id="socnet-share">
      





    </section>
    
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/_machine-learning/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/guitar_sound.jpg" alt="">
    
  </a>


    <div class="content">
      <h2 id="project-specifications">Project Specifications</h2>
<h4 id="some-notes-about-capstone">Some Notes about capstone</h4>
<p>This project was built mainly using Python, Python packages, Tensoflow and Raspberry Pi. All students in the Electrical Engineering program at Texas A&amp;M must complete a sponsored 2 semester capstone program. This is a very involved program usually involving 30-40 hrs of work each week. Each week included constant reporting to supervisors, oral presentations, and laboratory work. While the project is a group project, here I talk about my work on the project, and I how I helped the other members when needed. I received no help on the audio processing or machine learning aspects of this project.</p>
<h4 id="fsr-functional-system-requirements">FSR (Functional System Requirements)</h4>
<p>This is a shortened list, since the actual FSR is over 20 pages long. I also list the FSR of the other components since it is relevant to how I work with other systems.</p>
<p>There were 3 members and therefore 3 main subsystems.</p>
<ul>
<li>Machine Learning Inference System</li>
<li>Sound Location System</li>
<li>End User Screen and Transmission System</li>
</ul>
<p>The Requirements:</p>
<ol>
<li>The ASD (Active Shooter Detector) must differentiate when there is gunfire or not.</li>
<li>The ASD must source the location of said noise in 3D space.</li>
<li>The ASD must relay metrics from 1 and 2 to an officer through a screen.</li>
<li>The ASD must be mounted on a rover and capable of rotational and/or translational movement, controlled by the screen mentioned in 3.</li>
</ol>
<h4 id="icd-interface-control-document">ICD (Interface Control Document)</h4>
<p>This is a graphical representation. The actual ICD specifies all of the possible standards and interconnection issues for each interface and is over 20 pages long.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/system_overview.jpg"/> <figcaption>
            <h4>System Overview: note interaction with other subsystems</h4>
        </figcaption>
</figure>

<h2 id="research">Research</h2>
<h4 id="data-collection">Data collection</h4>
<p>My sponsor used the UMK-1 microphone to record gunfire at a local gun-range with various kinds of weapons. The reason that UMK-1 was chosen was because of its higher recording Frequency response compared to other usb plug in microphones.</p>
<p>This batch of .wav files, was then combined with a large open source of .wav files which was an accumulation of &ldquo;loud&rdquo; sounds. I say &ldquo;loud&rdquo; because there was not a defined limit as to what &ldquo;loud&rdquo; meant.</p>
<h4 id="the-model">The Model</h4>
<p>The next step was to pick a model for audio inference. Here we had to make a tough choice. Unlike common Linear Regressions, Audio has interesting constraints. For starters, it has more dimensionality, which restricts augmentation. How do you skew, flip, or stretch Audio without losing temporal fidelity?</p>
<p>The LSTM( Long and Short Term Memory) or BERT(Bidirectional Encoder Representation) are likely better suited to string and Natural Language Processing. But, due to time and frankly skill concerns, we decided to use a CNN image recognition model using batch spectrogram to convert a time based .wav file, to a pixel + color based .jpg image. In the end we went with a slightly modified VGG 16 Image Classification model.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/vgg16.jpg"/> <figcaption>
            <h4>VGG-16 Architecture: Our input sizes were slightly different, but the layers were organized like above</h4>
        </figcaption>
</figure>

<h2 id="process">Process</h2>
<h4 id="data-manipulation-and-the-batch-sprectrogram">Data Manipulation and the Batch Sprectrogram</h4>
<p>Any good Data Scientist knows that preparation is 90% of the game, so I spent quite a bit of time arranging, labeling, and cleaning the data.</p>
<p>In order to train a VGG-16 there needs to be well defined classes. The open source sound data we chose came with .wav files with names such as &ldquo;10120.wav&rdquo;. This number is associated with a row value of the &ldquo;1&rdquo; column from an included .csv file. That same row tells you much more detailed information such as the sound source. There were around 10k samples total of various lengths.</p>
<p>To accomplish the goal of organizing all of the files. I had to change the format of the wav file to &ldquo;sound_source + sample_number . wav&rdquo;. As an example &ldquo;10022.wav&rdquo; is actually &ldquo;Acoustic_guitar_22.wav&rdquo;(this is the actual image in the header of the article). This was accomplished using Pandas. With Pandas you can easily and non-recursively extract information into &ldquo;column&rdquo; based structures(i.e. DataFrames).</p>
<pre><code>#in what directory are our audio files?
path = './Ambient_Audio'
path_csv = './Ambient_Audio/_test_post_competition_scoring_clips.csv'
#Read the files into an array audio_files
audio_files = glob(path + '/*.wav') #adds anthing with extension .wav

####Read the CSV for the title content
df = pd.read_csv(path_csv)
df2 = df[['label','fname']]
print(df.head())
print(df2.head())

</code></pre><p>Once I had all of the &ldquo;correct&rdquo; names stored in a DataFrame, I used Librosa to convert all of the images into batch spectrogram form. Batch Spectrograms allow you to have time, power, and frequency information in one picture.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/120mel.jpg"/> <figcaption>
            <h4>MEL Spectrogram: This picture is a long timeline of multiple weapon shots</h4>
        </figcaption>
</figure>

<pre><code>#y refers to amplitude
#sr refers to sample rate
#load a specific file audio_files[i]
for i in range(len(audio_files)): #replace with length of
    y, sr = lr.core.load(audio_files[i])
    #where y is an array representing the audio time series
    #where sr is the sampling rate and a scalar

    #to find the magnitude spectogram S    
    S = lr.feature.melspectrogram(y=y, sr=sr)
    #extracts all of the mfcc from y and the sample rate
    #Takes the fast fourier transform I believe?
    S_dB = lr.power_to_db(S, ref =np.max)

        #open a second figure
    #plt.figure(i)
    lr.display.specshow(S_dB, sr=sr, fmax=44000)
    #x_axis = 'time', y_axis = 'mel',


    ###Attaching the file names to filenames######
    current_full_fname = audio_files[i]
    # pull just the last 8 elements of string also known as the label!
    #note that the .wav is also part of the label
    current_fname = current_full_fname[-12:]
    print(current_fname)
    ##See if that value is in the dataframe column 'label'
    df3 = df2.loc[df['fname'] == current_fname]
    final_fname = df3.iloc[0,0]


    plt.savefig( final_fname + '_'+ str(i)  + '_mel.jpg' , pad_inches = 0, bbox_inches='tight', dpi=300)
    #plt.title(final_fname)
    #add a title
</code></pre><h4 id="training-on-super-computer-clusters">Training on Super Computer Clusters</h4>
<p>No worries, the following sections are not nearly as long as the data manipulation sections.</p>
<p>Through my sponsor, I was able to gain access to the Olympus supercomputing clusters at A&amp;M in order to Train my model. The classes were chosen as:</p>
<ul>
<li>multiple_gunshots</li>
<li>not_gunshots</li>
<li>single_shots</li>
</ul>
<p>Because my model has RGB coloring, it is helpful to normalize data in order to prevent huge networks, which may or may not converge. The images were normalized from ~ 900x1400 px to 244px by 159px. Basically, I used matplotlib to scale down the image to as small as I could make it before I felt the images were too similar.</p>
<p>Finally,  I submitted everything in SLURM batch job files to Texas A&amp;M, after installing all of the dependencies in my virtual python environment. There was a lot of obscure Linux to get everything to run, but in a few words, it was simply installing all dependencies and praying you didn&rsquo;t mess up the batch file. It took a few tries before I understood the cluster.</p>
<p>At this point(Spring 2020), I had completed several ML projects already, so I had no issues with setting up or using TF(Tensorflow) or Keras. Below is just the layer and deployement code of one of the scripts.</p>
<pre><code>model = tf.keras.models.Sequential([
    #Convolution and Max Pooling Layers
    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu', input_shape = (244, 159, 3)),
    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #1st
    tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),
    tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #2nd
    tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'),
    tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'),
    tf.keras.layers.Conv2D(256, (3,3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #3rd
    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),
    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),
    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #4th
#    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),
#    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),
#    tf.keras.layers.Conv2D(512, (3,3), activation = 'relu'),    
    tf.keras.layers.MaxPooling2D(2,2),
    #5th
    #Lines the neurons up for connecting to dense Layers
    tf.keras.layers.Flatten(),
    #Drops some hidden layers in order to reduce overfitting
    tf.keras.layers.Dropout(.5),

    #Dense Layer fully connected
    tf.keras.layers.Dense(4096, activation = 'relu'),
    tf.keras.layers.Dense(4096, activation = 'relu'),

    #Final 3 output layer Gunshot, Multiple Shots, or no gunshot
    tf.keras.layers.Dense(3, activation = 'softmax')
])


# ### Compiling and Testing the Model
#print the summary of our classes and of our model
model.summary()
#compile
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Try and save every checkpoint which is every epoch
checkpoint = ModelCheckpoint(&quot;404_ver1_best_model.h5&quot;,monitor='loss', verbose =1,save_best_only = True, mode= 'auto', period = 1)

#save the history and set verbose to 1 to see live printout, for now at 0 if running on background server
history = model.fit_generator(train_generator, epochs=1, validation_data = validation_generator, verbose = 1, callbacks = [checkpoint])


#Save the weights and biases of trained model
model.save(&quot;404ver1.h5&quot;)
</code></pre><h5 id="model-layers-organized-the-classes-file-tree-by-hand">Model Layers: organized the classes&rsquo; file tree by hand.</h5>
<h4 id="integration-into-an-sbc">Integration into an SBC</h4>
<p>Generally, unless you have particular needs, the model weights and biases are saved to an &ldquo;.h5&rdquo; file. Once saved, this file can be used to pass appropriate inputs and predict a class. The problem for us was how to extract this prediction information &ldquo;real-time&rdquo;, and then display the prediction to an end user live constantly. And, to do this processing on a RPi4(Raspberry Pi 4)</p>
<p>To do this I generated a script that constantly makes predictions in 3 second intervals. Although this is not an ideal &ldquo;live prediction&rdquo; it was the best we could do with our power constraints on the RPi4.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/ML_Inferance.jpeg"/> <figcaption>
            <h4>Continuous Prediction Loop: All the above are in a single script </h4>
        </figcaption>
</figure>

<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Sat Feb 22 18:09:51 2020

@author: Daniel
&quot;&quot;&quot;
#---------------------RECORDING_RAW----------------
import sounddevice as sd
from scipy.io.wavfile import write

#---------------------SPECTROGRAM------------------
from glob import glob #For reading Files
import librosa as lr #For audio analysis
import librosa.display
from librosa import display

import numpy as np
import matplotlib.pyplot as plt #for plotting

#---------------------SPECTROGRAM------------------
from tensorflow.keras.models import load_model
from keras_preprocessing import image
from keras_preprocessing.image import load_img

model = load_model(&quot;404ver1.h5&quot;)

#How many times do you want to predict
for i in range(0,5):
    #Live Continous Prediction using Librosa and a Pretrained Model
    #Read in a audio signal and save it to .wav format
    #function 'record' takes in no inputs and outputs
    #the directory to where the .wav file is saved
    def record():
        #Our training audio is 44Khz
        fs = 44000
        #Let's say 3 seconds, we will pipeline
        seconds = 3

        #Record 3 seconds and save it
        current_recording = sd.rec(int(seconds*fs), samplerate = fs, channels = 2)
        #Wait until finished
        sd.wait()
        #Save as wav with approopriate size.
        write('last_3_seconds.wav', fs, current_recording)

        #Name the path of said recording as a string
        path_to_wav = './last_3_seconds.wav'
        return path_to_wav

    #After recording and saving the wav file, convert it into the
    #spectrogram image. 'spectro' has input path and output path_to_spectro
    def spectro(path_to_wav):
        #y refers to amplitude
        #sr refers to sample rate
        #load a specific file audio_files[i]
        y, sr = lr.core.load(path_to_wav)


        #extracts all of the mfcc from y and the sample rate
        #to find the magnitude spectogram S
        S = lr.feature.melspectrogram(y, sr=sr)
        #Takes the fast fourier transform I believe?
        S_dB = lr.power_to_db(S, ref=np.max)


        lr.display.specshow(S_dB,sr=sr, fmax=44000)
        plt.savefig('last_3_spectrogram.png', pad_inches = 0, bbox_inches='tight', dpi=300)
        plt.close()

        #Name the path of said figure
        path_to_spectro = './last_3_spectrogram.png'
        return path_to_spectro

    #After converting the wav to spectrogram, run an inference on it
    def predict(X):        
        #Load images from file
        img = load_img(X , target_size = (1402,913) )
        #Turn into array for parsing
        img = image.img_to_array(img)

        #expand the dimensions horizontally, turn into one big line
        x = np.expand_dims(img, axis=0)
        #now stack it all vertically
        img = np.vstack([x])

        #use built model subfunctions to produce a prediction
        classes = model.predict_classes(img, verbose = 0)
        percent_classes = model.predict(img, verbose = 0)
        print(classes, '\n', percent_classes)
        return classes, percent_classes


    path_to_wav = record()
    path_to_spectro = spectro(path_to_wav)    
    print(predict(path_to_spectro))

</code></pre><p>Finally, the prediction is displayed using RPi4&rsquo;s camera module. Using some Picamera functions, I helped the other subsystems to display live predictions on a screen.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/ML_Overlay.jpg"/> <figcaption>
            <h4>Live Camera Output: As you can see the probability of a gunshot was low at this point </h4>
        </figcaption>
</figure>

<h2 id="results">Results</h2>
<p>As a group we scored an 89%, mainly because they have a policy of not giving As unless the project is sold to a company. As for the ML here are the metrics. The truth is more data and cleaner data is needed. Because of University and Safety policies we only collected around 100 or so samples of gunfire. Therefore the model had trouble finding patterns converging to gunshot classification. Although a high accuracy is shown, it is highly skewed by the not gunshot class. Simply put, I should have trained either on LSTM or by only having single shots as my only class.</p>
<p>Also unfortunate, Covid-19 interfered with finishing the project completely. Mainly this hindered mounting the system on a rover and transmitting information OTA(Over the Air)  through Bluetooth. When progress was cancelled, everything was &ldquo;hardwired&rdquo; still.</p>
<figure>
    <img src="https://github.com/d9dch/Daniel_Chavez/img/ML_Metrics.jpg"/> <figcaption>
            <h4>Metrics: Includes loss value checkpoints </h4>
        </figcaption>
</figure>

<h4 id="conclusion">Conclusion</h4>
<p>In general terms, this is the project I am most proud of. And as it is my capstone, a high point in my college education. Using the latest technology in bleeding edge labs at Texas A&amp;M is always a pleasure. I would like to thank my for now unnamed sponsor, Samuel Beauchamp and Jacob Beckham for being amazing partners.</p>


    </div>
    <footer>
      <ul class="stats">
  <li class="categories">
    <ul>
    
      
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/categories/academic-project/">Academic Project</a></li>
        
      
    
    </ul>
  </li>
  <li class="tags">
    <ul>
    
      
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/python/">Python</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/pandas/">Pandas</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/tensorflow/">Tensorflow</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/audio-engineering/">Audio Engineering</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/embedded/">Embedded</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/linux/">Linux</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/rpi4/">RPi4</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/teensy/">Teensy</a></li>
        
          <li><a class="article-terms-link" href="https://github.com/d9dch/Daniel_Chavez/tags/keras/">Keras</a></li>
        
      
    
    </ul>
  </li>
</ul>

    </footer>
  </article>

  <div class="pagination">
  
    <a href="https://github.com/d9dch/Daniel_Chavez/projects/_molecule_linear_regression/" class="button"><div class="previous"><div>Coulomb Force predictions from chemical makeup</div></div></a>
  
  
</div>


      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent posts</h1>
      </header>
      
      <article class="mini-post">
        <section>
          
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/_machine-learning/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/guitar_sound.jpg" alt="">
    
  </a>


        </section>
        <header>
          <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/_machine-learning/">Capstone: Machine Learning for Active Shooter events</a></h2>
          <time class="published" datetime="">DOCUMENT LENGTH</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/_molecule_linear_regression/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/chemical_compound.JPG" alt="">
    
  </a>


        </section>
        <header>
          <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/_molecule_linear_regression/">Coulomb Force predictions from chemical makeup</a></h2>
          <time class="published" datetime="">DOCUMENT LENGTH</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/_microstrip_simulation/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/Microstrip.jpg" alt="">
    
  </a>


        </section>
        <header>
          <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/_microstrip_simulation/">Microstrip Simulation</a></h2>
          <time class="published" datetime="">DOCUMENT LENGTH</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/_solar_linear_regression/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/solar_panel_cover.jfif" alt="">
    
  </a>


        </section>
        <header>
          <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/_solar_linear_regression/">Solar Power: Linear Regression Utility</a></h2>
          <time class="published" datetime="">DOCUMENT LENGTH</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          
  <a href="https://github.com/d9dch/Daniel_Chavez/projects/traffic_stop_gender_bias/" class="image featured">
    
      <img src="https://github.com/d9dch/Daniel_Chavez/img/new_images/police_car.jpeg" alt="">
    
  </a>


        </section>
        <header>
          <h2><a href="https://github.com/d9dch/Daniel_Chavez/projects/traffic_stop_gender_bias/">Traffic Stop Racial and Gender Bias in Texas</a></h2>
          <time class="published" datetime="">DOCUMENT LENGTH</time>
        </header>
      </article>
      
      
        <footer>
          <a href="https://github.com/d9dch/Daniel_Chavez/projects/" class="button">See more</a>
        </footer>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="https://github.com/d9dch/Daniel_Chavez/categories">Categories</a></h1>
        </header>
        <ul>
          
            
          
          
          <li>
            
              <a href="https://github.com/d9dch/Daniel_Chavez/categories/academic-project/">academic-project<span class="count">5</span></a>
            
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>From small town Texas to Big Data dreams...</p>
      <footer>
        <a href="https://github.com/d9dch/Daniel_Chavez/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/pacollins/hugo-future-imperfect-slim" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/example" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>













<li><a href="//instagram.com/example" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>













<li><a href="mailto:example" target="_blank" title="Email" class="far fa-envelope"></a></li>

      </ul>
  
  <p class="copyright">
    
      &copy; 1
      
        Daniel Dominguez Chavez
      
    . <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.72.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="https://github.com/d9dch/Daniel_Chavez/js/highlight.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="https://github.com/d9dch/Daniel_Chavez/js/bundle.min.25d9afcb163201a20aa00263b3b4ac5384261d87c04a4e8bc87d63088246d0c2.js" integrity="sha256-JdmvyxYyAaIKoAJjs7SsU4QmHYfASk6LyH1jCIJG0MI="></script>
    <script src="https://github.com/d9dch/Daniel_Chavez/js/add-on.js"></script>
    </div>
  </body>
</html>
